# Transformer-based Sentiment Classification (BERT Tokenizer + Keras Transformer)

This project implements a modern NLP sentiment classifier that combines:

- A **BERT-style tokenizer** (HuggingFace) to convert text into token IDs  
- A custom **Transformer encoder** built with TensorFlow/Keras 
  (MultiHeadAttention + positional embeddings)  
- A classification head for **positive / negative** sentiment

The focus is on a clean, modular engineering pipeline similar to real-world
AI/ML projects.

---

## ðŸ§  Overview

- Input: short text reviews
- Output: sentiment label (negative / positive)
- Tokenization: BERT-style WordPiece tokenizer
- Model: Keras Transformer encoder with multi-head attention
- Training: Adam optimizer, early stopping, model checkpointing
- Evaluation: accuracy, classification report, confusion matrix

---

## ðŸ—‚ Project Structure

```text
transformer_bert
â”œâ”€â”€ data
â”‚   â””â”€â”€ sentiment_bert.csv
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ transformer_sentiment_best.keras
â”‚   â””â”€â”€ transformer_sentiment_final.keras
â”œâ”€â”€ notebooks
â”œâ”€â”€ reports
â”‚   â”œâ”€â”€ training_log.txt
â”‚   â””â”€â”€ evaluation_report.txt
â””â”€â”€ src
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ data_loader.py
    â”œâ”€â”€ tokenizer_builder.py
    â”œâ”€â”€ model_builder.py
    â”œâ”€â”€ train.py
    â””â”€â”€ evaluate.py
